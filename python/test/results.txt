ArgInfo(args=['tf_agent', 'train_env', 'eval_env', 'num_iterations', 'returns', 'losses', 'collect_episodes_per_iteration', 'log_interval', 'eval_interval', 'policy_checkpoint_interval', 'replay_buffer_capacity', 'num_eval_episodes', 'direc', 'verbose'], varargs=None, keywords=None, locals={'tf_agent': <tf_agents.agents.ppo.ppo_agent.PPOAgent object at 0x000001E402FDC948>, 'train_env': <tf_agents.environments.tf_py_environment.TFPyEnvironment object at 0x000001E402F0F748>, 'eval_env': <tf_agents.environments.tf_py_environment.TFPyEnvironment object at 0x000001E402F42948>, 'num_iterations': 1, 'returns': [], 'losses': [], 'collect_episodes_per_iteration': 30, 'log_interval': 100, 'eval_interval': 100, 'policy_checkpoint_interval': None, 'replay_buffer_capacity': 1000, 'num_eval_episodes': 100, 'direc': 'test/', 'verbose': False, 'f': <_io.TextIOWrapper name='test/results.txt' mode='w' encoding='cp1252'>})
step = 0: Average Collection Return = -17.0
step = 0: Average Collection Length = 4.54
step = 0: loss = 278.47906494140625
