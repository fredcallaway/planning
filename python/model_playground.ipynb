{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools as it\n",
    "from collections import Counter, defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.3)\n",
    "\n",
    "from toolz.curried import get, curry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.9628970551810276"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distributions import Normal\n",
    "from mouselab import MouselabEnv\n",
    "from exact import solve\n",
    "\n",
    "def make_env():\n",
    "    reward = Normal(3, 6).to_discrete(6).apply(int)\n",
    "    return MouselabEnv.new_symmetric([2,2], reward, cost=1)\n",
    "\n",
    "envs = Series([make_env() for _ in range(200)])\n",
    "env = envs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q, V, *_ = solve(env)\n",
    "V(env.init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies import SoftmaxPolicy\n",
    "from agents import run_episode\n",
    "\n",
    "def simulate(policy, envs, seed=None, repeat=1):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(1e5)\n",
    "    np.random.seed(seed)\n",
    "    for env_id, env in envs.items():\n",
    "        for _ in range(repeat):\n",
    "            trace = run_episode(policy, env)\n",
    "            for s, a, r in zip(*get(['states', 'actions', 'rewards'], trace)):\n",
    "                yield {'seed': seed, 'cost': abs(env.cost), 'env_id': env_id,\n",
    "                       'state': s, 'action': a, 'reward': r}\n",
    "\n",
    "optimal_policy = SoftmaxPolicy(Q)\n",
    "df_optimal = DataFrame(simulate(optimal_policy, envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of optimal policy actions under the optimal model\n",
      "0.333333     1\n",
      "0.500000    46\n",
      "1.000000    45\n",
      "dtype: int64\n",
      "\n",
      "Predictive power: 0.699\n"
     ]
    }
   ],
   "source": [
    "@curry\n",
    "def policy_model(policy, data):\n",
    "    return data.apply(lambda row: policy.action_distribution(row.state)[row.action], axis = 1)\n",
    "\n",
    "\n",
    "logp_optimal = np.log(policy_model(optimal_policy, df_optimal))\n",
    "print('Probabilities of optimal policy actions under the optimal model')\n",
    "print(np.exp(logp_optimal).value_counts().sort_index())\n",
    "print(f'\\nPredictive power: {np.exp(logp_optimal.mean()):.3f}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, Cat, -4, Cat, Cat, Cat, Cat)\n",
      "[ 0.     0.167  0.     0.167  0.167  0.167  0.167  0.167]\n"
     ]
    }
   ],
   "source": [
    "rand_policy = MouselabPolicy({'term_reward': 0})\n",
    "# NOTE: must run one episode before calling for action_distribution\n",
    "run_episode(rand_policy, env)\n",
    "env.reset()\n",
    "env.step(2)\n",
    "print(env._state)\n",
    "print(rand_policy.action_distribution(env._state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of random policy actions under the random model\n",
      "0.142857    20\n",
      "0.166667    18\n",
      "0.200000    15\n",
      "0.250000    14\n",
      "0.333333     8\n",
      "0.500000     6\n",
      "1.000000     3\n",
      "dtype: int64\n",
      "\n",
      "Predictive power: 0.219\n"
     ]
    }
   ],
   "source": [
    "df = DataFrame(simulate(rand_policy, envs))\n",
    "\n",
    "logp_rand = np.log(policy_model(rand_policy, df))\n",
    "print('Probabilities of random policy actions under the random model')\n",
    "print(np.exp(logp_rand).value_counts().sort_index())\n",
    "print(f'\\nPredictive power: {np.exp(logp_rand.mean()):.3f}', )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
