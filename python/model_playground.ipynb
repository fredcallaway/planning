{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools as it\n",
    "from collections import Counter, defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.3)\n",
    "\n",
    "from toolz.curried import get, curry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributions import Normal\n",
    "from mouselab import MouselabEnv\n",
    "from exact import solve\n",
    "\n",
    "def make_env():\n",
    "    reward = Normal(3, 6).to_discrete(6).apply(int)\n",
    "    return MouselabEnv.new_symmetric([2,2], reward, cost=1)\n",
    "\n",
    "env = make_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy\n",
    "\n",
    "Because we can compute the true optimal Q function, we can implement the optimal policy\n",
    "as a `SoftmaxPolicy` using `Q` as a preference function, and a very low temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SoftmaxPolicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b32daa162845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimal_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmaxPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SoftmaxPolicy' is not defined"
     ]
    }
   ],
   "source": [
    "Q, V, *_ = solve(env)\n",
    "V(env.init)\n",
    "optimal_policy = SoftmaxPolicy(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate rollouts\n",
    "\n",
    "Running rollouts of the optimal policy on 200 randomly selected environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies import SoftmaxPolicy\n",
    "from agents import run_episode\n",
    "\n",
    "def simulate(policy, envs, seed=None, repeat=1):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(1e5)\n",
    "    np.random.seed(seed)\n",
    "    for env_id, env in envs.items():\n",
    "        for _ in range(repeat):\n",
    "            trace = run_episode(policy, env)\n",
    "            for s, a, r in zip(*get(['states', 'actions', 'rewards'], trace)):\n",
    "                yield {'seed': seed, 'cost': abs(env.cost), 'env_id': env_id,\n",
    "                       'state': s, 'action': a, 'reward': r}\n",
    "\n",
    "envs = Series([make_env() for _ in range(200)])\n",
    "df_optimal = DataFrame(simulate(optimal_policy, envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "`SoftMaxPolicy` selects actions by sampling from action probabilities generated by its\n",
    "`action_distribution` method. We can use that method as a likelihood model as well.\n",
    "I print summary statistics to get an idea of the fit quality. \"Predictive power\" (a name I made up)\n",
    "is very similar to _perplexity_, but it's in probability units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_model(policy, data):\n",
    "    return data.apply(lambda row: policy.action_distribution(row.state)[row.action], axis = 1)\n",
    "\n",
    "logp_optimal = np.log(policy_model(optimal_policy, df_optimal))\n",
    "print('Probabilities of optimal policy actions under the optimal model')\n",
    "print(np.exp(logp_optimal).value_counts().sort_index())\n",
    "print(f'\\nPredictive power: {np.exp(logp_optimal.mean()):.3f}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Policy\n",
    "\n",
    "Now we can do the same thing again (without redefining functions) for the random policy.\n",
    "Note that the random policy is uniform oveer *legal* actions only. As we would hope, it\n",
    "less predictive of itself compared to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_policy = MouselabPolicy({'term_reward': 0})\n",
    "# NOTE: must run one episode before calling for action_distribution\n",
    "run_episode(rand_policy, env)\n",
    "env.reset()\n",
    "env.step(2)\n",
    "print(env._state)\n",
    "print(rand_policy.action_distribution(env._state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(simulate(rand_policy, envs))\n",
    "logp_rand = np.log(policy_model(rand_policy, df))\n",
    "print('Probabilities of random policy actions under the random model')\n",
    "print(np.exp(logp_rand).value_counts().sort_index())\n",
    "print(f'\\nPredictive power: {np.exp(logp_rand.mean()):.3f}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best first\n",
    "\n",
    "This is just a start, not sure it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_first_pref(state, action, satisfice=9):\n",
    "    if action == env.term_action:\n",
    "        if env.expected_term_reward(state) > satisfice:\n",
    "            return 1e10  # always terminate\n",
    "        else:\n",
    "            return -1e10  # never terminate (unless it's the only option)\n",
    "    q = env.node_quality(action, state)  # note backwards arguments!  plz don't ask why...\n",
    "    return q.expectation()  # node_quality is a distribution, we just want the mean\n",
    "\n",
    "best_first_policy = SoftmaxPolicy(best_first_pref)\n",
    "run_episode(best_first_policy, env)\n",
    "df = DataFrame(simulate(best_first_policy, envs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_best_first = np.log(policy_model(best_first_policy, df))\n",
    "print('Probabilities of best_firstom policy actions under the best_firstom model')\n",
    "print(np.exp(logp_best_first).value_counts().sort_index())\n",
    "print(f'\\nPredictive power: {np.exp(logp_best_first.mean()):.3f}', )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
