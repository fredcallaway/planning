
\section{Experiment 1}
\label{sec:experiments}

\fl{Here, we evaluated the bounded-optimal planning model against previous models of planning and test its predictions in an environment where the variance is high and constant across levels.}

%\fl{The structure of each experiment section is
%\begin{enumerate}
%    \item We describe experiment-unique structure
%    \item We derive qualitative predictions.
%    \item Description of human data
%    \item Test of qualitative predictions
%    \item Test Quantitative model comparisons between the bounded-optimal strategy and previously proposed heuristics
%\end{enumerate}}
% Experiment 1: effect of increasing vs. decreasing variance
% Experiment 2: effect of low vs. high variance
% Experiment 3: branching and depth

\subsection{Methods}
\paragraph{Description of the Experiment}
\fl{TODO: Fred}

\paragraph{Verbal protocol analysis}
\fl{TODO: Paul and Sayan}

\subsection{Models}

\paragraph{Derivation and characterization of the bounded-optimal planning strategy}
\fl{TODO: Fred and Falk}

\paragraph{Models of classical planning strategies}
\fl{TODO: Priyam}

\paragraph{Directed cognition model}
\fl{TODO: Sayan}


\subsection{Results}

\paragraph{Qualitative predictions}
\fl{TODO: Falk}

\paragraph{Model Comparisons}
\fl{TODO: Fred and Priyam}

\paragraph{Verbal protocol analysis}
\fl{TODO: Paul and Sayan}

\section{Experiment 2: Adaptation to the structure of the environment}

\fl{Here we test our model's prediction that people's planning strategies should change adaptively with the structure of the environment.}

\subsection{Methods}

Each participant was randomly assigned to one of three conditions: The first condition was identical to Experiment 1. In the second condition the variability of the reward distribution was scaled down by a factor of 10 (($\sigma_R=1$)). In the third condition the variability $\sigma_R$ of the rewards increased outwardly from $X$ in the first step, to $Y$ in the second step, and $Z$ in the third step.

We computed the bounded-optimal planning strategy for each of the three environments using the meta-level reinforcement learning algorithm developed by \cite{LiederCallawayGulKruegerGriffiths2017}. We characterized the behavior of the three resulting planning strategies to derive the predictions presented below.

\fl{TODO: Falk and Fred}

\subsection{Results}

\paragraph{Reward variability increases planning}
Bounded optimality predicts that people should perform significantly less planning when the variability of the rewards is low (Condition 2) than when it is high (Condition 1). Concretely, ...

We found that ...
\fl{TODO: Fred, Falk, and Priyam}

Verbal protocol analysis revealed that ...
\fl{TODO: Paul and Sayan}

\paragraph{Planning focuses on the most informative outcomes}
Bounded optimality predicts that people's planning strategy should prioritize state-action pairs that might yield large positive or large negative outcomes over state-action pairs whose rewards will be rather negligible. Concretely, since the variance of the reward distribution in Condition 3 increases outwardly, bounded optimality predicts that participants in Condition 3 should start by inspecting the possible end-states until they find one that is good enough and only resort to inspecting the more immediate rewards if the two best possible end-states are identical.

We found that ...
\fl{TODO: Fred, Falk, and Priyam}

Verbal protocol analysis revealed that ...
\fl{TODO: Paul and Sayan}

%\subsection{Experiment 2: low vs. high variability}
%\fl{Here, the basic prediction is that higher variability in the reward distribution favors more planning.}
%\subsection{Experiment 2: increasing vs. decreasing variability}
%\fl{Here, the basic prediction is that backward planning is optimal when the variability in the achievable reward increases into future than when it decreases.}
%\subsection{Experiment 3: branching and depth}